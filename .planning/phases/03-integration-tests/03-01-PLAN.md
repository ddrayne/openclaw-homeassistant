---
phase: 03-integration-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_config_flow.py
  - tests/test_conversation.py
autonomous: true

must_haves:
  truths:
    - "Config flow accepts valid configuration and creates entry"
    - "Config flow shows correct errors for auth/connection/timeout failures"
    - "Conversation entity sends messages and returns responses"
    - "Conversation entity returns helpful error messages on failures"
  artifacts:
    - path: "tests/test_config_flow.py"
      provides: "Config flow integration tests"
      min_lines: 80
    - path: "tests/test_conversation.py"
      provides: "Conversation entity integration tests"
      min_lines: 60
---

<objective>
Create focused integration tests for the two main user-facing flows:
1. Config flow - user sets up the integration
2. Conversation - user sends messages and gets responses

Target: ~15 tests total that exercise real code paths with mocked WebSocket.
</objective>

<context>
Key files to test:
- custom_components/clawd/config_flow.py - setup/options flow
- custom_components/clawd/conversation.py - message handling
- custom_components/clawd/gateway_client.py - client layer

Existing fixtures in tests/conftest.py:
- mock_config_entry - creates mock config entry
- mock_websocket - creates mock WebSocket
- mock_websocket_connect - patches websockets.connect
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create config flow tests</name>
  <files>tests/test_config_flow.py</files>
  <action>
Create tests/test_config_flow.py with focused tests for the config flow.

Tests needed (~5):
1. test_valid_config_creates_entry - valid input → entry created
2. test_auth_error_shows_invalid_auth - GatewayAuthenticationError → "invalid_auth"
3. test_connection_error_shows_cannot_connect - GatewayConnectionError → "cannot_connect"
4. test_timeout_error_shows_timeout - GatewayTimeoutError → "timeout"
5. test_duplicate_config_aborts - same host:port → abort

Approach:
- Mock validate_connection() to control success/failure
- Use HA test framework if available, otherwise create minimal mocks
- Each test should verify the error key in result["errors"]

Pattern:
```python
from unittest.mock import patch, AsyncMock

async def test_auth_error_shows_invalid_auth(hass):
    """Auth failure should show invalid_auth error."""
    with patch(
        "custom_components.clawd.config_flow.validate_connection",
        side_effect=GatewayAuthenticationError("bad token"),
    ):
        result = await hass.config_entries.flow.async_init(
            DOMAIN, context={"source": "user"}, data=valid_input
        )
        assert result["errors"]["base"] == "invalid_auth"
```

Handle standalone mode (no HA framework) by skipping tests that require full HA.
  </action>
  <verify>pytest tests/test_config_flow.py -v</verify>
  <done>Config flow tests cover success and error paths</done>
</task>

<task type="auto">
  <name>Task 2: Create conversation tests</name>
  <files>tests/test_conversation.py</files>
  <action>
Create tests/test_conversation.py with focused tests for the conversation entity.

Tests needed (~5):
1. test_send_message_returns_response - happy path
2. test_connection_error_returns_helpful_message - GatewayConnectionError
3. test_timeout_error_returns_helpful_message - GatewayTimeoutError
4. test_agent_error_returns_helpful_message - AgentExecutionError
5. test_emoji_stripping_when_enabled - verify integration with strip_emojis

Approach:
- Create mock gateway_client that returns controlled responses
- Create mock conversation input
- Call _async_handle_message directly (avoids HA framework complexity)
- Verify the response speech contains expected text

Pattern:
```python
async def test_connection_error_returns_helpful_message():
    """Connection error should return user-friendly message."""
    mock_client = AsyncMock()
    mock_client.send_agent_request = AsyncMock(
        side_effect=GatewayConnectionError("connection failed")
    )

    entity = ClawdConversationEntity(mock_config_entry, mock_client)
    result = await entity._async_handle_message(mock_input, mock_chat_log)

    assert "connecting" in result.response.speech["plain"]["speech"].lower()
```

These tests focus on the conversation entity logic, not HA framework integration.
  </action>
  <verify>pytest tests/test_conversation.py -v</verify>
  <done>Conversation tests cover success and error paths</done>
</task>

</tasks>

<verification>
1. Run all new tests: `pytest tests/test_config_flow.py tests/test_conversation.py -v`
2. Run full test suite: `pytest tests/ -v` (should pass ~160 tests)
</verification>

<success_criteria>
- ~10 integration tests created
- Config flow success and error paths tested
- Conversation success and error paths tested
- Tests run fast (mocked, no real network)
- Tests work in standalone mode (no HA framework required)
</success_criteria>

<output>
After completion, create `.planning/phases/03-integration-tests/03-01-SUMMARY.md`
</output>
