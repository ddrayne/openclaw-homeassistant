---
phase: 01-test-infrastructure-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - tests/test_fixtures.py
autonomous: true

must_haves:
  truths:
    - "pytest command runs without errors"
    - "Coverage report shows custom_components/clawd modules"
    - "Fixtures inject correctly into test functions"
    - "No 'Event loop is closed' errors in test output"
  artifacts:
    - path: "tests/test_fixtures.py"
      provides: "Smoke tests verifying fixture functionality"
      min_lines: 30
  key_links:
    - from: "tests/test_fixtures.py"
      to: "tests/conftest.py"
      via: "fixture injection"
      pattern: "def test_.*hass.*mock_config_entry.*mock_websocket"
---

<objective>
Verify test infrastructure works by creating smoke tests that exercise all fixtures and running pytest.

Purpose: Confirm all Phase 1 infrastructure works end-to-end before subsequent phases build on it.
Output: tests/test_fixtures.py with smoke tests, verified pytest run with coverage output.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-test-infrastructure-foundation/01-RESEARCH.md
@.planning/phases/01-test-infrastructure-foundation/01-01-SUMMARY.md
@.planning/phases/01-test-infrastructure-foundation/01-02-SUMMARY.md

@pyproject.toml
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install test dependencies</name>
  <files></files>
  <action>
Install the test dependencies from pyproject.toml:

Run: `pip install -e ".[test]"`

This installs the project in editable mode with all test dependencies.

If pip install fails due to version conflicts with pytest-homeassistant-custom-component, try:
1. First attempt: version as specified (>=0.13.200)
2. Fallback: pin to a specific known-working version like 0.13.180

Note: pytest-homeassistant-custom-component has strict version requirements - may need to adjust based on Python version.
  </action>
  <verify>Run `pip list | grep pytest` to confirm pytest packages are installed</verify>
  <done>Test dependencies installed successfully</done>
</task>

<task type="auto">
  <name>Task 2: Create fixture smoke tests</name>
  <files>tests/test_fixtures.py</files>
  <action>
Create tests/test_fixtures.py with smoke tests for each fixture:

1. **test_hass_fixture_available**:
   - Takes `hass` parameter (from pytest-homeassistant-custom-component)
   - Asserts `hass is not None`
   - Asserts `hasattr(hass, 'config_entries')`

2. **test_mock_config_entry_has_correct_domain**:
   - Takes `mock_config_entry` parameter
   - Asserts `mock_config_entry.domain == "clawd"`
   - Asserts `mock_config_entry.data["host"] == "localhost"`
   - Asserts `mock_config_entry.data["port"] == 8765`

3. **test_mock_websocket_is_async_mock**:
   - Takes `mock_websocket` parameter
   - Asserts `mock_websocket.send` is AsyncMock
   - Asserts `mock_websocket.recv` is AsyncMock
   - Asserts `mock_websocket.close` is AsyncMock

4. **test_mock_websocket_connect_patches_correctly**:
   - Takes `mock_websocket_connect` parameter
   - Unpacks `mock_connect, mock_ws = mock_websocket_connect`
   - Asserts `mock_connect` is callable
   - Asserts `mock_ws is not None`

5. **test_async_cleanup_runs_without_error** (async):
   - Takes `async_cleanup` parameter
   - Create a simple asyncio.Task that completes immediately
   - Assert no exceptions raised
   - This verifies the cleanup pattern works

Add imports:
- unittest.mock (AsyncMock)
- pytest
- asyncio
- homeassistant.core (HomeAssistant) for type hints
- pytest_homeassistant_custom_component.common (MockConfigEntry) for type hints
  </action>
  <verify>Run `python -c "import ast; ast.parse(open('tests/test_fixtures.py').read())"` to verify syntax</verify>
  <done>test_fixtures.py contains 5 smoke tests covering all fixtures</done>
</task>

<task type="auto">
  <name>Task 3: Run pytest and verify infrastructure</name>
  <files></files>
  <action>
Run the test suite and verify all Phase 1 success criteria:

1. Run: `pytest tests/test_fixtures.py -v`
   - All 5 tests should pass
   - No "Event loop is closed" errors in output

2. Run: `pytest --cov=custom_components/clawd --cov-report=term-missing`
   - Coverage report should appear
   - Should show custom_components/clawd modules (even if 0% coverage since no actual code tested yet)

3. Verify no warnings about:
   - Event loop issues
   - Fixture not found
   - Import errors

If tests fail:
- Check error messages for fixture issues
- Verify conftest.py imports are correct
- Check pytest-homeassistant-custom-component version compatibility
  </action>
  <verify>pytest exit code 0, coverage report visible in output</verify>
  <done>All smoke tests pass, coverage report shows custom_components/clawd modules, no async cleanup errors</done>
</task>

</tasks>

<verification>
1. `pip list | grep pytest` shows all test packages installed
2. `pytest tests/test_fixtures.py -v` passes all tests
3. `pytest --cov` shows coverage report
4. No "Event loop is closed" errors in any test output
5. test_fixtures.py is committed to repo
</verification>

<success_criteria>
- Test dependencies installed successfully
- All 5 fixture smoke tests pass
- Coverage report displays after test run
- No async cleanup errors (validates INFRA-04)
- Single CLI command (`pytest`) runs tests (validates INFRA-05)
</success_criteria>

<output>
After completion, create `.planning/phases/01-test-infrastructure-foundation/01-03-SUMMARY.md`
</output>
